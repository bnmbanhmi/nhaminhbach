---
tags: sprint
status: #active
id: 250925
timeframe: 2025-09-25 to 2025-10-09
epic: [[E1]]
---

# S7: The Transformation Engine

## üéØ Sprint Goal
To build and deploy a robust, automated data transformation pipeline. The objective is to systematically convert unstructured, raw text from scraped posts into structured, validated data entities ready for human review. This sprint creates the core "refinery" of our data factory, bridging the gap between raw collection and quality control.

**Definition of Done for Sprint:**
- [ ] Pydantic data contracts mirror PostgreSQL schema exactly
- [ ] LLM transformation logic processes >95% of posts without critical errors
- [ ] Cloud Run service deployed and accessible via secure endpoint
- [ ] Event-driven transformation triggers automatically on new raw data
- [ ] End-to-end pipeline: raw entry ‚Üí structured 'pending_review' entry operates with zero manual intervention

## üìã Sprint Backlog & Task Progress

### üöÄ Primary Tasks
- [ ] **[[25081919_define_pydantic_data_contracts]]**: Create a set of Pydantic models in Python that serve as the strict, non-negotiable schema for our data. These models must mirror the structure of the `listings` and `attributes` tables in our PostgreSQL database. This is the blueprint for the LLM's output. **STATUS: ACTIVE**

- [ ] **[[build_core_transformation_logic]]**: Develop the central Python function that:
  1. Accepts raw text from a scraped post as input
  2. Constructs a detailed, zero-shot prompt for the Gemma 3n E4B LLM
  3. Uses the `Instructor` library to call the LLM API and enforce the Pydantic data contract, ensuring the output is a validated, structured Python object

- [ ] **[[deploy_transformer_as_a_cloud_service]]**: Package the transformation logic into a lightweight web service (using Flask or FastAPI) and deploy it to Google Cloud Run. This service will expose a single, secure endpoint (e.g., `/transform`).

- [ ] **[[implement_transformation_trigger]]**: Establish an event-driven mechanism (e.g., using a Firestore/PubSub trigger or a database trigger) that automatically invokes the `/transform` endpoint whenever a new record is inserted into the `raw_scraped_data` table.

## üìä Sprint Metrics & Health
- **Planned Story Points:** 16 points
- **Completed Story Points:** 0 points
- **Velocity:** TBD
- **Burn Rate:** Not started

### üìà Success Metrics
- **Metric 1:** The `Transformer` service successfully processes >95% of incoming raw posts without critical errors
- **Metric 2:** The output data correctly populates the `listings` and `listing_attributes` tables with the status set to `'pending_review'`
- **Metric 3:** The entire pipeline, from a new raw entry to a structured `'pending_review'` entry, executes automatically with zero manual intervention

## üö´ Sprint Scope & Boundaries
**In Scope:**
- Pydantic model definitions matching database schema
- Core LLM transformation logic with Instructor library
- Cloud Run deployment of transformation service
- Automated triggering mechanism for new raw data

**Out of Scope:**
- Human review interface (reserved for S8)
- Public-facing features
- Performance optimization beyond basic functionality
- Advanced error handling and retry logic

## ‚ùó Strategic Context
This sprint is a strategic insertion into our roadmap. It addresses the critical need for data quality *before* public exposure. It is a direct dependency for Sprint 8 and completes the automated portion of our data factory defined in Epic [[E1]].
