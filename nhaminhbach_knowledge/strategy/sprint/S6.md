---
tags: sprint
status: #active
id: 250820
timeframe: 2025-08-16 to today
---

# S6: The Local-to-Cloud Bridge

## üéØ Goal
-   To create a secure and robust bridge enabling the local scraper to submit its raw data to the cloud-based QC pipeline. This makes the local scraping strategy viable and integrates it with our existing data processing system, allowing for a high-velocity, human-in-the-loop data acquisition workflow.

## Tasks
-   [[create_the_ingestion_api]] ‚Äî Done (implemented `ingest_scraped_data` in `packages/functions/main.py`)
-   [[secure_the_ingestion_api_with_api_key]] ‚Äî Done (via `X-API-Key` header, `INGEST_API_KEY` env)
-   [[implement_duplicate_prevention_logic]] ‚Äî Done (app-level check + DB unique indexes)
-   [[upgrade_the_local_scraper_to_submit_data]] ‚Äî Next

### Notes / How to use
- Endpoint: `ingest_scraped_data` (Firebase Function, Python). Auth via `X-API-Key`.
- Body contract:
    - `source` (string, required)
    - `source_post_id` (string, optional)
    - `source_url` (string, optional)
    - `scraped_at` (ISO8601 string, optional)
    - `raw` (object, required) ‚Äî untouched JSON payload from scraper
- Dedup rules:
    - Duplicate if same `source_url`, or same `(source, source_post_id)`.
    - Returns `{ duplicate: true, id }` for duplicates; else creates and returns `{ duplicate: false, id }`.

### Ops
- Local env: add `INGEST_API_KEY` in `packages/functions/.env.yaml`.
- DB: new table `raw_scraped_data` created lazily by the API; SQL reference at `packages/functions/sql/001_create_raw_scraped_data.sql`.

### Next
- Wire `packages/scraper/main.py` to POST to `ingest_scraped_data` with the correct payload and API key.

## ‚è™ Retrospective
-   **What has gone well?**
    -   The strategic pivot to a local-first scraping approach was immediately validated. The speed of debugging scraper logic (`--headful` mode) and the quality of data from a "real" browser environment proved far superior for this stage of the project.
    -   Our existing backend architecture demonstrated its flexibility. Adding a new, secure API endpoint was a straightforward task because the foundational patterns (database connection, configuration) were already well-established.
    -   The decision to make the ingestion API "dumb" (it only saves raw data, it doesn't parse) was a key success. It enforces a clean separation of concerns: Scrapers scrape, Ingesters save, and the QC process enriches.

-   **What has gone wrong?**
    -   None yet; pending end-to-end test once scraper submission is wired.

