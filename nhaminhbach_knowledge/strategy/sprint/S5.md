---
tags: sprint
status: #done
id: 250819
timeframe: 2025-09-10 to 2025-09-24
---

# S5: The Data Factory

## ðŸŽ¯ Goal
-   To build and deploy the fully automated, scalable, end-to-end data scraping pipeline on Google Cloud Platform. The objective is to create a "factory" that can be scheduled to run, automatically fetch new data from Facebook, and prepare it for the manual QC process, thus completing the core data acquisition loop of the business model.

## âœ… Tasks Completed
-   [[containerize_the_scraper_script]]: Successfully packaged the feature-complete Python Playwright scraper into a robust, multi-stage Docker container.
-   [[automate_builds_with_cloud_build]]: Established a full CI/CD pipeline using a `cloudbuild.yaml` configuration and a GitHub trigger to automatically build and push new scraper images to Artifact Registry.
-   [[deploy_the_orchestration_layer]]: Architected and deployed the two-part management system: the `orchestrate_scrapes` "Orchestrator" (as a Cloud Function) and the `scrape_job_executor` "Executor" (as a Cloud Run service).
-   [[wire_the_pipeline_with_pubsub_and_scheduler]]: Successfully connected all microservices using Pub/Sub as the message bus and Cloud Scheduler as the master timer, creating a complete, event-driven system.
-   [[achieve_first_fully_automated_end_to_end_scrape]]: Successfully executed the entire chain, from the scheduled trigger to the final execution of multiple `scrape-job` instances on Cloud Run.

## âª Retrospective
-   **What went well?**
    -   The final architecture itselfâ€”Scheduler -> Orchestrator -> Pub/Sub -> Executor -> Jobâ€”proved to be incredibly robust and scalable once fully assembled.
    -   Our methodical, evidence-based debugging approach (e.g., using "Canary Logs" and "Canary Screenshots") was the key to solving otherwise invisible environmental problems.
    -   The "No-Magic Protocol" (eliminating `ENTRYPOINT` from the scraper `Dockerfile`) was a critical breakthrough that solved the most persistent execution error.

-   **What went wrong?**
    -   This sprint exposed the profound difference between local and cloud environments (the **"Sterile Room Problem"**). The scraper, which worked perfectly locally, initially failed on Cloud Run because the "clean" container environment was immediately flagged by Facebook's anti-bot measures.
    -   We were plagued by a persistent "Ghost in the Machine" issue where code and configuration changes did not seem to apply after deployment. This was ultimately traced to the ambiguity of `gcloud run deploy --source` and was only solved by enforcing a strict, explicit **Build-then-Deploy** workflow.
    -   We significantly underestimated the complexity of service-to-service IAM permissions and authentication (OIDC), leading to multiple, time-consuming debugging cycles (`401`, `403`, permission denied on secrets, etc.).

-   **Key Principle Learned:** The "Sterile Room Problem" taught us the crucial principle of [[environment_divergence]]: never assume local success translates directly to cloud success. It also reinforced our [[zero_warning_tolerance]] principle. The entire sprint was a lesson in the superiority of explicit commands over implicit framework "magic".

-   **Key Processes Created:** This sprint forced the creation and formalization of our most powerful debugging protocols: the [[canary_log_protocol]], the [[canary_screenshot_protocol]], and the strict [[explicit_build_then_deploy_protocol]].

-   **Key Documents Involved:** This sprint was the ultimate implementation and validation of the [[core_architecture]]. The final, working `gcloud` commands and `Dockerfile` configurations are now canonical parts of our #system knowledge base.